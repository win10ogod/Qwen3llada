# Qwen3LLada 專案簡介（擴充版）

本專案在不大幅改動 Qwen3 主架構的前提下，將其改造成擴散式語言模型（Diffusion LM, DLM）以進行非自回歸的遮罩預測學習。模型沿用 Qwen3 的層、權重與頭部（LM head），在 qwen3llada 中以非因果注意力（non‑causal）進行前向；同時參考 LLaDA 的訓練設計與 dLLM‑RL 的強化策略，提供更穩定、可控的訓練與推理流程。

## 目標與設計原則
- 目標：在 Qwen3 架構上實現 DLM 與 LLaDA 類似的訓練/推理，同時吸收 dLLM‑RL 的優化（遮罩排程、損失、取樣等），讓「收斂更穩定、取樣更一致」。
- 原則：不侵入 Qwen3 主體；以「包裝 + 前向掩碼（非因果/自訂 bias）」達成架構轉換。
- 兼容：保持 HuggingFace 風格 API 與檢查點格式（`save_pretrained`/`from_pretrained`）。

## 關鍵功能
- 非因果注意力 + 可選的 Additive Attention Bias
  - 模型預設非因果（`attention_mask=None`），同時支援傳入自訂 additive bias（[B,1,Q,K] 形狀，0 表可見、-inf 表遮罩），便於實作「訓練時的 Block 注意力偏置」或其他高階可視策略。
- 三種訓練損失（可切換）
  - guidelines：依 LLaDA 原文，對 masked 位置做 CE/p_mask，(b*l) 正規化。
  - dllm：改編自 dLLM‑RL 的 LLaDA loss（masked 以 noisy==mask_id 判定），穩定性佳。
  - soft_ce：加入 label smoothing 的 masked CE，再除以 p_mask（受 dLLM‑RL 的 soft‑CE 啟發），提升穩定度。
  - 所有 CE 都在 float32 上計算（即使啟用 bf16/fp16），避免數值不穩。
- 遮罩/噪聲排程（dLLM‑RL 風格，預訓練專用）
  - schedule：uniform/cosine/power/sigmoid；
  - noise_type：mask 或 random_replace；
  - min_masking_rate：夾住過小的 p_mask，穩定 1/p 權重；
  - mask_contiguous_region_prob：以機率使用連續 span 遮罩；
  - reweight_cap：限制 1/p_mask 的上限；
  - 隨機裁短 1% 序列（LLaDA 原建議）。
- 訓練時 Block 注意力偏置（可選）
  - 與半自回歸 padding 的取樣一致：prompt 彼此可見；答案區每個 block 僅互相可見（仍可見 prompt）。
  - 在訓練與推理皆可開啟，提升一致性與穩定性。
- 推理（取樣）
  - remasking：low_confidence / random / random_topk（帶隨機性的低置信選擇）；
  - Block 生成：`--block_length` 控制分塊；可加 `--block_attention` 對應訓練偏置；
  - 支援 CFG（classifier‑free guidance）。
- 優化器與學習率排程
  - Optimizer：AdamW、AdamW8bit、Lion8bit（自動 fallback）；
  - LR：linear / cosine + warmup（transformers 調用）。
- 檢查點與恢復
  - 週期保存 `{output_dir}/checkpoints/step-{N}`，含 model、tokenizer、optimizer.pt、trainer_state.json（epoch/global_step/epoch_batch）；
  - 從 latest 或指定 checkpoint 恢復，支援 epoch 內 batch 跳過，真正從中斷位置續訓。
- HF datasets 整合
  - 預訓練：以一般文本欄位（自動探測）tokenize，非 streaming 模式會 pack 成固定長度；
  - SFT：支援 HF 會話格式（見下）並產出 `prompt_lengths`，按 LLaDA SFT loss 設計訓練。

## 目錄與核心檔案
- `modeling/qwen3llada/`
  - `configuration_qwen3llada.py`：`Qwen3LLadaConfig`（預設 `use_cache=False`）。
  - `modeling_qwen3llada.py`：`Qwen3LLadaModel`/`Qwen3LLadaForMaskedLM`；非因果 + additive bias。
- `scripts/`
  - `train.py`：訓練主腳本；支援 pretrain/SFT、block 偏置、三種 loss、LR scheduler、checkpoint。
  - `inference.py`：推理主腳本；支援 block_attention、random_topk remasking、CFG。
  - `loss.py`：guidelines/dllm/soft_ce 三種損失與前向加噪。
  - `convert.py`：Qwen3 → Qwen3LLada 權重對應（拷貝而不改值）。
  - `analyze.py`：Qwen3 vs Qwen3LLada 權重鍵名/形狀對照。
- `config.yaml`：完整配置（模型、訓練、資料集、評估、優化器、噪聲與 block 偏置）。

## SFT 數據格式（HF 導入）
支援如下會話格式（每筆至少需一組 human→gpt）：
```json
[
  {
    "conversations": [
      {"from": "human", "value": "人类指令"},
      {"from": "gpt",   "value": "模型回答"}
    ],
    "system": "系统提示词（选填）"
  }
]
```
- 映射規則：BOS + system + human 為 prompt；gpt 為 answer；最後追加 EOS（若存在），同時記錄 `prompt_lengths`。
- 訓練時 SFT loss 不對 prompt 區加噪，並以答案長度正規化（依 LLaDA SFT 設計）。

## 快速開始
1) 轉換（可選）
```bash
python scripts/convert.py --src demo_model/Qwen3-0.6B-Base --dst out/qwen3llada-converted
```
2) 預訓練
```bash
python scripts/train.py --config config.yaml
```
3) SFT（HF 會話格式）
- 將 `config.yaml` 中：
  - `training.mode: sft`
  - `dataset.name: <你的HF資料集>`（含 conversations/system）
```bash
python scripts/train.py --config config.yaml
```
4) 推理
```bash
python scripts/inference.py \
  --model out/qwen3llada-base \
  --prompt "What is diffusion LM?" \
  --steps 128 --gen_length 128 --block_length 32 \
  --remasking random_topk --block_attention
```

## 配置要點（config.yaml）
- `training.loss_impl`: `guidelines` | `dllm` | `soft_ce`
- `training.noise`: `schedule`(uniform/cosine/power/sigmoid), `noise_type`(mask/random_replace), `min_masking_rate`, `mask_contiguous_region_prob`, `reweight_cap`, `random_length_prob`
- `training.block_attention.enabled`: true/false；`block_length`: 與推理一致
- `optim.name`: `adamw` | `adamw8bit` | `lion8bit`（自動 fallback 到 AdamW）
- `evaluation.do_eval_loss`: true → 每 `eval_every_steps` 跑 eval loss（同訓練 loss 路徑）

## 注意事項
- Diffusion 架構不使用 KV-cache；`Qwen3LLadaConfig` 預設 `use_cache=False`。
- 非 streaming 模式可 pack 成固定長度（利於 (b*l) 正規化穩定 loss）；SFT 路徑不建議 pack。
- Windows 上 `bitsandbytes` 可能不可用；程式會自動 fallback。
- CE 在 float32 計算更穩（即使啟用 bf16/fp16）。

## 後續可擴充
- 訓練更進階的遮罩策略（兩段遮罩、comp/random 混合），複製自 dLLM‑RL 的 RL/SFT 流程，做成開關。
- 分層或交替層級的 block 注意力策略（類 SDAR 的 per‑layer 規劃）。
- 更豐富的取樣策略（動態閾值 unmask、置信‑溫度混合）。

